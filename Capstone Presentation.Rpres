Data Science Specialization SwiftKey Capstone
========================================================
author: Andrew Delos Santos
date: 12/21/2019
autosize: true

Abstract
========================================================
For this capstone, I created a predictive text model based on the copora provided 
by Swiftkey and Johns Hopkins which were public sources from a web crawler. To help 
form my predictive text algorithm, I used an N-gram model basis which uses Markov
Chains to efficiently store the n-gram and used Katz's back-off Model which 
estimated the conditional probability of a word given a certain n-gram. As a 
result, I managed to create a 3-gram model which predicted the next word based 
on the previous 1, 2 or 3 words. To learn more about the models, look at the links
below.

- [linked phrase](http://en.wikipedia.org/wiki/Katz%27s_back-off_model)

- [linked phrase](https://en.wikipedia.org/wiki/N-gram)


Sampled, Cleaned and Merged Twitter, Blogs, and News Phrasetable
========================================================
After completing the procedures of cleaning the data set in the milestone report,
I merged the three texts. After merging these 3 texts, to prevent the
computer from consuming too much memory, I sampled 20% of the size of the dataset to 
create the prediction model. Shown below are the top 5 3-gram phrases.
```{r echo = FALSE}
three_gram_english_lines_df <- readRDS("three_gram_all_english_texts.rds")
head(three_gram_english_lines_df, 5)
```

Slide With Plot
========================================================

```{r, echo=FALSE}
plot(cars)
```
